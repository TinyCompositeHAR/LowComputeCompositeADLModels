{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-27T04:00:30.822394Z",
     "start_time": "2024-09-27T03:58:50.474222Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, GlobalAveragePooling1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load the preprocessed data\n",
    "X_train = np.loadtxt(\"../Datasets/CAPP Dataset/SubjectIndependent50PercentOverlap/X_train.txt\")\n",
    "y_train = np.loadtxt(\"../Datasets/CAPP Dataset/SubjectIndependent50PercentOverlap/y_train.txt\")\n",
    "X_test = np.loadtxt(\"../Datasets/CAPP Dataset/SubjectIndependent50PercentOverlap/X_test.txt\")\n",
    "y_test = np.loadtxt(\"../Datasets/CAPP Dataset/SubjectIndependent50PercentOverlap/y_test.txt\")\n",
    "\n",
    "# Reshape the input data for Conv1D layers\n",
    "n_timestep = 100\n",
    "n_features = 9\n",
    "X_train = X_train.reshape(X_train.shape[0], n_timestep, n_features)\n",
    "X_test = X_test.reshape(X_test.shape[0], n_timestep, n_features)\n",
    "\n",
    "# One-hot encode the labels\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "\n",
    "def create_lstm_model(n_timesteps, n_features, n_outputs):\n",
    "    model = Sequential([\n",
    "        # LSTM layer with return_sequences=False to reduce complexity\n",
    "        LSTM(64, activation=\"tanh\", input_shape=(n_timesteps, n_features)),\n",
    "\n",
    "        # Dense layers for classification\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(n_outputs, activation=\"softmax\")  # For multi-class classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "n_outputs = y_train.shape[1]\n",
    "conv_mlp = create_lstm_model(n_timestep, n_features, n_outputs)\n",
    "conv_mlp.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = conv_mlp.fit(X_train, y_train, epochs=50, batch_size=512, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = conv_mlp.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes, digits=5))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 09:58:50.656057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 09:58:50.666910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 09:58:50.670306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 09:58:50.678884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 09:58:51.186676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727409552.077640  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.110207  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.114809  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.118830  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.121612  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.124284  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.232871  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.233990  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727409552.235035  116757 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:59:12.236108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5816 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:0e:00.0, compute capability: 8.6\n",
      "/home/denuvo-drm/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 09:59:13.939769: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 9ms/step - accuracy: 0.2711 - loss: 2.3821 - val_accuracy: 0.4392 - val_loss: 1.7604\n",
      "Epoch 2/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.5772 - loss: 1.2755 - val_accuracy: 0.5583 - val_loss: 1.4986\n",
      "Epoch 3/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.6700 - loss: 0.9978 - val_accuracy: 0.6000 - val_loss: 1.3339\n",
      "Epoch 4/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.7313 - loss: 0.8230 - val_accuracy: 0.6182 - val_loss: 1.3622\n",
      "Epoch 5/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.7601 - loss: 0.7254 - val_accuracy: 0.6409 - val_loss: 1.3411\n",
      "Epoch 6/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.7889 - loss: 0.6445 - val_accuracy: 0.6377 - val_loss: 1.3399\n",
      "Epoch 7/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.7981 - loss: 0.6087 - val_accuracy: 0.6312 - val_loss: 1.3789\n",
      "Epoch 8/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8091 - loss: 0.5772 - val_accuracy: 0.6430 - val_loss: 1.3640\n",
      "Epoch 9/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8355 - loss: 0.4968 - val_accuracy: 0.6489 - val_loss: 1.3218\n",
      "Epoch 10/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8500 - loss: 0.4631 - val_accuracy: 0.6567 - val_loss: 1.3340\n",
      "Epoch 11/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8601 - loss: 0.4313 - val_accuracy: 0.6598 - val_loss: 1.3348\n",
      "Epoch 12/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8654 - loss: 0.4185 - val_accuracy: 0.6722 - val_loss: 1.3324\n",
      "Epoch 13/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8769 - loss: 0.3816 - val_accuracy: 0.6792 - val_loss: 1.2584\n",
      "Epoch 14/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8792 - loss: 0.3709 - val_accuracy: 0.6798 - val_loss: 1.2597\n",
      "Epoch 15/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8872 - loss: 0.3542 - val_accuracy: 0.6662 - val_loss: 1.3022\n",
      "Epoch 16/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8905 - loss: 0.3374 - val_accuracy: 0.6767 - val_loss: 1.3054\n",
      "Epoch 17/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8953 - loss: 0.3260 - val_accuracy: 0.6848 - val_loss: 1.2981\n",
      "Epoch 18/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8940 - loss: 0.3314 - val_accuracy: 0.6890 - val_loss: 1.3031\n",
      "Epoch 19/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.9037 - loss: 0.3019 - val_accuracy: 0.6879 - val_loss: 1.2812\n",
      "Epoch 20/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.9031 - loss: 0.2965 - val_accuracy: 0.6785 - val_loss: 1.3120\n",
      "Epoch 21/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.9100 - loss: 0.2834 - val_accuracy: 0.6745 - val_loss: 1.3564\n",
      "Epoch 22/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.9128 - loss: 0.2737 - val_accuracy: 0.6810 - val_loss: 1.3561\n",
      "Epoch 23/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9164 - loss: 0.2635 - val_accuracy: 0.6984 - val_loss: 1.2789\n",
      "Epoch 24/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9171 - loss: 0.2609 - val_accuracy: 0.6939 - val_loss: 1.2815\n",
      "Epoch 25/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9167 - loss: 0.2566 - val_accuracy: 0.7122 - val_loss: 1.2521\n",
      "Epoch 26/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9210 - loss: 0.2475 - val_accuracy: 0.6819 - val_loss: 1.3755\n",
      "Epoch 27/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9237 - loss: 0.2437 - val_accuracy: 0.6943 - val_loss: 1.3403\n",
      "Epoch 28/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9244 - loss: 0.2339 - val_accuracy: 0.6915 - val_loss: 1.3228\n",
      "Epoch 29/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9270 - loss: 0.2278 - val_accuracy: 0.6969 - val_loss: 1.3364\n",
      "Epoch 30/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9303 - loss: 0.2198 - val_accuracy: 0.6798 - val_loss: 1.4440\n",
      "Epoch 31/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9303 - loss: 0.2192 - val_accuracy: 0.6783 - val_loss: 1.4235\n",
      "Epoch 32/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9350 - loss: 0.2029 - val_accuracy: 0.6977 - val_loss: 1.3931\n",
      "Epoch 33/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9337 - loss: 0.2049 - val_accuracy: 0.7013 - val_loss: 1.3976\n",
      "Epoch 34/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9344 - loss: 0.2032 - val_accuracy: 0.6981 - val_loss: 1.3293\n",
      "Epoch 35/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9381 - loss: 0.1901 - val_accuracy: 0.7098 - val_loss: 1.3359\n",
      "Epoch 36/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9410 - loss: 0.1849 - val_accuracy: 0.7079 - val_loss: 1.3652\n",
      "Epoch 37/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9406 - loss: 0.1816 - val_accuracy: 0.6916 - val_loss: 1.4505\n",
      "Epoch 38/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9431 - loss: 0.1742 - val_accuracy: 0.7044 - val_loss: 1.4100\n",
      "Epoch 39/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9451 - loss: 0.1723 - val_accuracy: 0.7050 - val_loss: 1.3398\n",
      "Epoch 40/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9311 - loss: 0.2099 - val_accuracy: 0.7024 - val_loss: 1.4088\n",
      "Epoch 41/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9472 - loss: 0.1658 - val_accuracy: 0.6859 - val_loss: 1.5655\n",
      "Epoch 42/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9453 - loss: 0.1658 - val_accuracy: 0.7008 - val_loss: 1.4387\n",
      "Epoch 43/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9502 - loss: 0.1532 - val_accuracy: 0.7135 - val_loss: 1.3934\n",
      "Epoch 44/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9488 - loss: 0.1577 - val_accuracy: 0.7079 - val_loss: 1.4398\n",
      "Epoch 45/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9508 - loss: 0.1515 - val_accuracy: 0.7075 - val_loss: 1.4958\n",
      "Epoch 46/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9496 - loss: 0.1474 - val_accuracy: 0.6953 - val_loss: 1.5184\n",
      "Epoch 47/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9498 - loss: 0.1516 - val_accuracy: 0.7078 - val_loss: 1.4612\n",
      "Epoch 48/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9551 - loss: 0.1385 - val_accuracy: 0.7016 - val_loss: 1.4873\n",
      "Epoch 49/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9481 - loss: 0.1586 - val_accuracy: 0.7046 - val_loss: 1.5883\n",
      "Epoch 50/50\n",
      "\u001B[1m156/156\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9534 - loss: 0.1404 - val_accuracy: 0.7060 - val_loss: 1.4563\n",
      "\u001B[1m616/616\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.74853   0.82201   0.78355       927\n",
      "           1    0.95568   0.95361   0.95464       927\n",
      "           2    0.71275   0.71970   0.71621       924\n",
      "           3    0.89873   0.56670   0.69510       877\n",
      "           4    0.72237   0.58034   0.64361       946\n",
      "           5    0.54261   0.60957   0.57415       940\n",
      "           6    0.76592   0.82839   0.79593       944\n",
      "           7    0.66963   0.79368   0.72640       950\n",
      "           8    0.70876   0.28917   0.41075       951\n",
      "           9    0.60563   0.27832   0.38137       927\n",
      "          10    0.79559   0.68306   0.73504      1322\n",
      "          11    0.57096   0.92730   0.70676       564\n",
      "          12    0.75407   0.94319   0.83810       933\n",
      "          13    0.59732   0.90685   0.72024       934\n",
      "          14    0.62046   0.60000   0.61006       940\n",
      "          15    0.48033   0.50000   0.48997       928\n",
      "          16    0.91569   0.97089   0.94248       962\n",
      "          17    0.30430   0.27492   0.28886       953\n",
      "          18    0.63684   0.63953   0.63819       946\n",
      "          19    0.67745   0.65966   0.66844       952\n",
      "          20    0.54397   0.69874   0.61172       956\n",
      "\n",
      "    accuracy                        0.67386     19703\n",
      "   macro avg    0.67751   0.67836   0.66341     19703\n",
      "weighted avg    0.68097   0.67386   0.66370     19703\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T04:02:55.807110Z",
     "start_time": "2024-09-27T04:02:54.607922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(conv_mlp)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(\"SubjectDependent/GeneratedTFLiteFilesAndOGModels/lstm_mlp_model_deploy.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TensorFlow Lite model has been saved as 'lstm_mlp_model_deploy.tflite'\")\n",
    "\n",
    "# Optional: Save the Keras model\n",
    "conv_mlp.save(\"GeneratedTFLiteFilesAndOGModels/lstm_mlp_model_deploy.h5\")\n",
    "print(\"Keras model has been saved as 'lstm_mlp_model_deploy.h5'\")\n",
    "\n",
    "# If you still need to generate a C header file\n",
    "try:\n",
    "    from everywhereml.code_generators.tensorflow import convert_model\n",
    "\n",
    "    c_header = convert_model(conv_mlp, X_test, y_test, model_name='lstm_mlp_model_deploy')\n",
    "\n",
    "    with open(\"SubjectDependent/GeneratedHeaderFiles/lstm_mlp_model.h\", \"w\") as file:\n",
    "        file.write(c_header)\n",
    "\n",
    "    print(\"C header file has been saved as GeneratedHeaderFiles/lstm_mlp_model.h'\")\n",
    "except ImportError:\n",
    "    print(\"everywhereml library not found. Skipping C header file generation.\")\n"
   ],
   "id": "b52dcf7c1dced840",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxhdje4lq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxhdje4lq/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpxhdje4lq'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 9), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 21), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  129068357453072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357453952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357452368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357456768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357458176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357455008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357459760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357456416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  129068357461344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1727409774.906458  116757 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1727409774.906470  116757 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-27 10:02:54.906653: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxhdje4lq\n",
      "2024-09-27 10:02:54.907035: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-27 10:02:54.907042: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpxhdje4lq\n",
      "2024-09-27 10:02:54.910585: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-09-27 10:02:54.911157: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-27 10:02:54.930032: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpxhdje4lq\n",
      "2024-09-27 10:02:54.936939: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 30288 microseconds.\n",
      "2024-09-27 10:02:54.944521: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "loc(callsite(callsite(fused[\"CudnnRNNV3:\", \"sequential_1/lstm_1/CudnnRNNV3@__inference_function_44032\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_44079\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): error: 'tf.CudnnRNNV3' op is neither a custom op nor a flex op\n",
      "error: failed while converting: 'main': \n",
      "Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \n",
      "Custom ops: CudnnRNNV3\n",
      "Details:\n",
      "\ttf.CudnnRNNV3(tensor<?x100x9xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<19200xf32>, tensor<?xi32>) -> (tensor<?x100x64xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<*xf32>, tensor<*xi8>) : {T = f32, device = \"\", direction = \"unidirectional\", dropout = 0.000000e+00 : f32, input_mode = \"linear_input\", is_training = true, num_proj = 0 : i64, rnn_mode = \"lstm\", seed = 0 : i64, seed2 = 0 : i64, time_major = false}\n",
      "\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"CudnnRNNV3:\", \"sequential_1/lstm_1/CudnnRNNV3@__inference_function_44032\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_44079\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.CudnnRNNV3' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"CudnnRNNV3:\", \"sequential_1/lstm_1/CudnnRNNV3@__inference_function_44032\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_44079\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_CUSTOM_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \nCustom ops: CudnnRNNV3\nDetails:\n\ttf.CudnnRNNV3(tensor<?x100x9xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<19200xf32>, tensor<?xi32>) -> (tensor<?x100x64xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<*xf32>, tensor<*xi8>) : {T = f32, device = \"\", direction = \"unidirectional\", dropout = 0.000000e+00 : f32, input_mode = \"linear_input\", is_training = true, num_proj = 0 : i64, rnn_mode = \"lstm\", seed = 0 : i64, seed2 = 0 : i64, time_major = false}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConverterError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Convert the model to TensorFlow Lite format\u001B[39;00m\n\u001B[1;32m      2\u001B[0m converter \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mlite\u001B[38;5;241m.\u001B[39mTFLiteConverter\u001B[38;5;241m.\u001B[39mfrom_keras_model(conv_mlp)\n\u001B[0;32m----> 3\u001B[0m tflite_model \u001B[38;5;241m=\u001B[39m \u001B[43mconverter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Save the TFLite model\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGeneratedTFLiteFilesAndOGModels/lstm_mlp_model_deploy.tflite\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1231\u001B[0m, in \u001B[0;36m_export_metrics.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(convert_func)\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1230\u001B[0m   \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m-> 1231\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_and_export_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1183\u001B[0m, in \u001B[0;36mTFLiteConverterBase._convert_and_export_metrics\u001B[0;34m(self, convert_func, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_conversion_params_metric()\n\u001B[1;32m   1182\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mprocess_time()\n\u001B[0;32m-> 1183\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1184\u001B[0m elapsed_time_ms \u001B[38;5;241m=\u001B[39m (time\u001B[38;5;241m.\u001B[39mprocess_time() \u001B[38;5;241m-\u001B[39m start_time) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1744\u001B[0m, in \u001B[0;36mTFLiteKerasModelConverterV2.convert\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;129m@_export_metrics\u001B[39m\n\u001B[1;32m   1732\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m \n\u001B[1;32m   1735\u001B[0m \u001B[38;5;124;03m  Returns:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;124;03m      Invalid quantization parameters.\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1744\u001B[0m   saved_model_convert_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_as_saved_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1745\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m saved_model_convert_result:\n\u001B[1;32m   1746\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saved_model_convert_result\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1725\u001B[0m, in \u001B[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1721\u001B[0m   graph_def, input_tensors, output_tensors \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1722\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_keras_to_saved_model(temp_dir)\n\u001B[1;32m   1723\u001B[0m   )\n\u001B[1;32m   1724\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msaved_model_dir:\n\u001B[0;32m-> 1725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTFLiteKerasModelConverterV2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph_def\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_tensors\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1729\u001B[0m   shutil\u001B[38;5;241m.\u001B[39mrmtree(temp_dir, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1466\u001B[0m, in \u001B[0;36mTFLiteConverterBaseV2.convert\u001B[0;34m(self, graph_def, input_tensors, output_tensors)\u001B[0m\n\u001B[1;32m   1459\u001B[0m   logging\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   1460\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing new converter: If you encounter a problem \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1461\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease file a bug. You can opt-out \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1462\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby setting experimental_new_converter=False\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1463\u001B[0m   )\n\u001B[1;32m   1465\u001B[0m \u001B[38;5;66;03m# Converts model.\u001B[39;00m\n\u001B[0;32m-> 1466\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43m_convert_graphdef\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1467\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_def\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1468\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1469\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1470\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconverter_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1471\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1473\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimize_tflite_model(\n\u001B[1;32m   1474\u001B[0m     result,\n\u001B[1;32m   1475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_quant_mode,\n\u001B[1;32m   1476\u001B[0m     _build_conversion_flags(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconverter_kwargs)\u001B[38;5;241m.\u001B[39mdebug_options,\n\u001B[1;32m   1477\u001B[0m     quant_io\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_new_quantizer,\n\u001B[1;32m   1478\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:212\u001B[0m, in \u001B[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    210\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     report_error_message(\u001B[38;5;28mstr\u001B[39m(converter_error))\n\u001B[0;32m--> 212\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m converter_error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Re-throws the exception.\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m    214\u001B[0m   report_error_message(\u001B[38;5;28mstr\u001B[39m(error))\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001B[0m, in \u001B[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    204\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m ConverterError \u001B[38;5;28;01mas\u001B[39;00m converter_error:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m converter_error\u001B[38;5;241m.\u001B[39merrors:\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:1014\u001B[0m, in \u001B[0;36mconvert_graphdef\u001B[0;34m(input_data, input_tensors, output_tensors, **kwargs)\u001B[0m\n\u001B[1;32m   1011\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1012\u001B[0m     model_flags\u001B[38;5;241m.\u001B[39moutput_arrays\u001B[38;5;241m.\u001B[39mappend(util\u001B[38;5;241m.\u001B[39mget_tensor_name(output_tensor))\n\u001B[0;32m-> 1014\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_flags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconversion_flags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSerializeToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebug_info_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSerializeToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdebug_info\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_mlir_converter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_mlir_converter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/miniconda3/envs/CompositeADLRecognition/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:376\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\u001B[0m\n\u001B[1;32m    368\u001B[0m         conversion_flags\u001B[38;5;241m.\u001B[39mguarantee_all_funcs_one_use \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    369\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m convert(\n\u001B[1;32m    370\u001B[0m             model_flags,\n\u001B[1;32m    371\u001B[0m             conversion_flags,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m             enable_mlir_converter,\n\u001B[1;32m    375\u001B[0m         )\n\u001B[0;32m--> 376\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converter_error\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _run_deprecated_conversion_binary(\n\u001B[1;32m    379\u001B[0m     model_flags\u001B[38;5;241m.\u001B[39mSerializeToString(),\n\u001B[1;32m    380\u001B[0m     conversion_flags\u001B[38;5;241m.\u001B[39mSerializeToString(),\n\u001B[1;32m    381\u001B[0m     input_data_str,\n\u001B[1;32m    382\u001B[0m     debug_info_str,\n\u001B[1;32m    383\u001B[0m )\n",
      "\u001B[0;31mConverterError\u001B[0m: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"CudnnRNNV3:\", \"sequential_1/lstm_1/CudnnRNNV3@__inference_function_44032\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_44079\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.CudnnRNNV3' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"CudnnRNNV3:\", \"sequential_1/lstm_1/CudnnRNNV3@__inference_function_44032\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_44079\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_CUSTOM_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \nCustom ops: CudnnRNNV3\nDetails:\n\ttf.CudnnRNNV3(tensor<?x100x9xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<19200xf32>, tensor<?xi32>) -> (tensor<?x100x64xf32>, tensor<?x1x64xf32>, tensor<?x1x64xf32>, tensor<*xf32>, tensor<*xi8>) : {T = f32, device = \"\", direction = \"unidirectional\", dropout = 0.000000e+00 : f32, input_mode = \"linear_input\", is_training = true, num_proj = 0 : i64, rnn_mode = \"lstm\", seed = 0 : i64, seed2 = 0 : i64, time_major = false}\n\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "addeaa0e8bcc4125"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
